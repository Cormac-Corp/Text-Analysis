{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "text = \"\"\n",
    "pdfFileObj = open('/Users/mguduri/Downloads/vaccine.pdf', 'rb')\n",
    "pdfReader = PyPDF2.PdfFileReader(pdfFileObj)\n",
    "\n",
    "#print(pdf.numPages)\n",
    "for page in range(pdf.numPages):\n",
    "    pageObj = pdfReader.getPage(page).extractText()\n",
    "    text += pageObj\n",
    "    \n",
    "#text = text.replace('\\n', ' ')\n",
    "#text = text.replace('in˜uenza', 'influenza')\n",
    "text = text.replace('In˚uenza', 'influenza') \n",
    "text = text.replace('in˚uenza', 'influenza')\n",
    "text = text.replace('in˜uenza', 'influenza')\n",
    "#print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "tok = word_tokenize(text)\n",
    "#print(tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Cleaning the text ###\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "#nltk.download()\n",
    "\n",
    "punctuations = ['(',')',';',':','[',']',',', '...', '.', '&']\n",
    "\n",
    "words2 = open('/Users/mguduri/Downloads/words.txt', 'r')\n",
    "words3 = []\n",
    "words3 = words2.read()\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "keywords = [word for word in s if not word in stop_words and not word in punctuations and not word in words3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "cnt = Counter()\n",
    "words = []\n",
    "count1 = []\n",
    "for word in keywords:\n",
    "    cnt[word] += 1\n",
    "top = cnt.most_common(10)\n",
    "for (a, b) in top:\n",
    "    words.append(a)\n",
    "    count1.append(b)\n",
    "print(words, count1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import texttable as tt\n",
    "tab = tt.Texttable()\n",
    "\n",
    "words = []\n",
    "numbers = []\n",
    "headings = ['Word','Number']\n",
    "tab.header(headings)\n",
    "for a, b in top:\n",
    "    words.append(a)\n",
    "    numbers.append(b)\n",
    "print(words, numbers)\n",
    "Word = words\n",
    "Number = numbers\n",
    "\n",
    "for row in zip(Word, Number):\n",
    "    tab.add_row(row)\n",
    "\n",
    "s = tab.draw()\n",
    "print (s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#arr = np.array(top)\n",
    "string = []\n",
    "numbers = []\n",
    "for a, b in top:\n",
    "    string.append(a)\n",
    "    numbers.append(b)\n",
    "    \n",
    "\n",
    "#plt.xticks(xn, string)\n",
    "#plt.figure(figsize=(20,10))\n",
    "#print(string, numbers)\n",
    "#plt.bar(string, numbers, facecolor='blue', alpha=0.5)\n",
    "#xn = range(len(string))\n",
    "bar_width = 0.8\n",
    "plt.bar(string, numbers, bar_width)\n",
    "#plt.rcParams['figure.figsize'] = (13, 11)\n",
    "\n",
    "#plt.xticks(xn, string)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Generating wordcloud from a given PDF Doc ###\n",
    "\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.pyplot as plt\n",
    "stopwords = set(STOPWORDS)\n",
    "\n",
    "def show_wordcloud(data, title = None):\n",
    "    wordcloud = WordCloud(\n",
    "        background_color='white',\n",
    "        stopwords=stopwords,\n",
    "        max_words=200,\n",
    "        max_font_size=40, \n",
    "        scale=3,\n",
    "        random_state=1 # chosen at random by flipping a coin; it was heads\n",
    "    ).generate(str(data))\n",
    "\n",
    "    fig = plt.figure(1, figsize=(12, 12))\n",
    "    plt.axis('off')\n",
    "    if title: \n",
    "        fig.suptitle(title, fontsize=20)\n",
    "        fig.subplots_adjust(top=2.3)\n",
    "\n",
    "    plt.imshow(wordcloud)\n",
    "    plt.show()\n",
    "\n",
    "show_wordcloud(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Creaing Summary of a given document ###\n",
    "\n",
    "from nltk.corpus import reuters\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem import PorterStemmer\n",
    "import nltk\n",
    "import datetime, re, sys\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def tokenize_and_stem(text):\n",
    "    tokens = [word for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "    filtered_tokens = []\n",
    "    # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)\n",
    "    stemmer=PorterStemmer()\n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z]', token):\n",
    "            filtered_tokens.append(token)\n",
    "    stems = [stemmer.stem(t) for t in filtered_tokens]\n",
    "    return stems\n",
    "\n",
    "token_dict = {}\n",
    "for article in reuters.fileids():\n",
    "    token_dict[article] = reuters.raw(article)\n",
    "        \n",
    "tfidf = TfidfVectorizer(tokenizer=tokenize_and_stem, stop_words='english', decode_error='ignore')\n",
    "print('building term-document matrix... [process started: ' + str(datetime.datetime.now()) + ']')\n",
    "sys.stdout.flush()\n",
    "\n",
    "tdm = tfidf.fit_transform(token_dict.values()) # this can take some time (about 60 seconds on my machine)\n",
    "#print('done! [process finished: ' + str(datetime.datetime.now()) + ']')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from random import randint\n",
    "\n",
    "feature_names = tfidf.get_feature_names()\n",
    "print('TDM contains ' + str(len(feature_names)) + ' terms and ' + str(tdm.shape[0]) + ' documents')\n",
    "\n",
    "print ('first term: ' + feature_names[0])\n",
    "print ('last term: ' + feature_names[len(feature_names) - 1])\n",
    "\n",
    "for i in range(0, 4):\n",
    "    print ('random term: ' + feature_names[randint(1,len(feature_names) - 2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from __future__ import division\n",
    "new_list = \"\"\n",
    "article_id = randint(0, tdm.shape[0] - 1)\n",
    "article_text = text\n",
    "#for b in a.split('.'):\n",
    "sent_scores = []\n",
    "for sentence in nltk.sent_tokenize(a):\n",
    "    score = 0\n",
    "    sent_tokens = tokenize_and_stem(sentence)\n",
    "    for token in (t for t in sent_tokens if t in feature_names):\n",
    "        score += tdm[article_id, feature_names.index(token)]\n",
    "    sent_scores.append((score / len(sent_tokens), sentence))\n",
    "\n",
    "summary_length = int(math.ceil(len(sent_scores) / 5))\n",
    "sent_scores.sort(key=lambda sent: sent[0])\n",
    "print ('*** SUMMARY ***')\n",
    "for summary_sentence in sent_scores[:summary_length]:\n",
    "    \n",
    "    new_list += summary_sentence[1]+'\\n'\n",
    "\n",
    "print(new_list)\n",
    "    #print ('\\n*** ORIGINAL ***')\n",
    "    #print (article_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
